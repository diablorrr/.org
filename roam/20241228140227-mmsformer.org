:PROPERTIES:
:ID:       a16d35ec-de65-434e-98e5-e4ce6ebd0b29
:END:
#+title: MMSFormer
#+filetags: paper

* 动机
** 先前工作的问题
1. 模态融合数量的限制：有的无法同时融合超过两种模态；有的超过了，但融合策略复杂
2. 对特定模态的依赖：有的多模态方法是为特定的模态对设计，限制了通用性和适应性
** 改进的内容
1. 提出新型融合策略：论文提出新的融合块，处理任意数量的输入模态，有效融合不同模态组合信息，减少对特定模态的依赖，增加通用性和适应性。


* 模型 :ATTACH:
:PROPERTIES:
:ID:       ebaf8eec-46e9-4ceb-b43a-5114f1b31813
:END:
[[attachment:_20241228_140533screenshot.png]]
# 注：每个Fusion Block输出结果都是直接传给MLP Layers
- Patch Embedding：将输出切分为块
- Mix Transformer Blocks
  - Spatial Reduction：减少计算消耗
  - Multi-Head Attention
  - mixer layer(MLP+Conv+MLP)：全局特征、局部特征、全局特征
  - Overlap Patch Merging：将重叠的图像块合并成一个完整的图像
    # 对于重叠区域的图像块，合并它们的方式为，取平均值，加权平均值等
- Fusion Block
  - Concat：通道维度连接特征
  - Linear Fusion Layer：通道维度连接特征，并减小通道维度
  - 捕获和混合多尺度的模块(2个Linear Projection Layer+3个Conv)：2个LPL捕获全局信息，3个Conv提取不同尺度的特征
    # 不同大小的卷积，融合块可以关注局部模式，并捕获更大的空间结构
  - Channel Attention：增强神经网络对不同通道的关注度(选择重要的特征通道，抑制不重要的特征)
- MLP Layers：
  [[attachment:_20241228_140601screenshot.png]]
  将不同形状的融合特征变成相同的通道维度，上采样到原始输入形状的1/4大小，沿通道维度连接产生最终融合特征，产生预测分割


* 实验
** 数据集
1. **MCubeS数据集**：
   - 包含500组图像，涵盖42个==街头==场景。
   - 具有==四种不同的模态==：RGB（彩色图像）、AoLP（线性偏振角）、DoLP（线性偏振度）和NIR（近红外）。
   - ==有标注==。数据集中有20个类别标签，对应不同的材质。
   - 数据集分为训练集（302个图像组）、验证集（96个图像组）和测试集（102个图像组）。
2. **FMB数据集**：
   - 数据集覆盖了==不同光照和天气条件==下的场景（如雨、雾和强光）。
   - 包含1500对==RGB-红外==图像对。
   - ==有标注==。14个类别标签。
   - 训练集和测试集分别包含1220对和280对图像对。
3. **PST900数据集**：
   - 包含894对==RGB-热成像==图像对。
   - 数据集分为训练集和测试集。
   - ==有标注==。5个类别标签。
** 基线模型
1. **DRConv**：使用动态区域感知卷积来适应不同区域的特征，提升分割精度。
2. **DDF (Resnet-101)** : 这是一种动态区域感知卷积网络，用于多模态图像分割。
3. **TransFuser**：利用==Transformer结构==进行多模态数据融合，适用于自动驾驶等场景。
4. **DeepLabv3**：通过==空洞卷积和多尺度特征融合==实现高效准确的语义分割。
5. **MMTM**：多模态传输模块，优化不同模态间的特征融合，提升分割性能。
6. **FuseNet**：专为RGB-D分割设计，通过融合深度信息来增强分割效果。
7. **MCubeSNet** : 这是一个为多模态材料分割设计的模型，能够==整合四种不同的模态==来增强分割的准确性。
8. **CBAM** : 这是一个==注意力机制==模型，通过聚焦==通道和空间信息==来增强特征表示。
9. **CMNeXt** : 这是一种多模态融合方法，它通过==自查询中心和并行池混合模块==来实现不同模态数据的融合。
** 评估指标
1. **交并比（IoU）**：
   - IoU 是衡量预测分割区域与真实分割区域重叠程度的指标。
   - 计算方法：IoU = (TP) / (TP + FP + FN)
     # 其中TP表示真正例（正确预测的像素数），FP表示假正例（错误预测为正类的像素数），FN表示假负例（错误预测为负类的像素数）。
   - IoU 的值范围在0到1之间，值越接近1表示预测的分割结果与真实情况越接近。
2. **平均交并比（mIoU）**：
   - mIoU 是所有类别IoU的平均值，用于多类别分割任务的性能评估。
   - 计算方法：对每个类别分别计算IoU，然后求平均值。
   - mIoU 能够更全面地反映模型在不同类别上的性能。
** 具体实验 :ATTACH:
:PROPERTIES:
:ID:       8ba5bc9c-bad8-442a-a421-b2a026fcf5ee
:END:
[[attachment:_20241228_140636screenshot.png]]
[[attachment:_20241228_140654screenshot.png]]
[[attachment:_20241228_140710screenshot.png]]


** 模型预测的定性分析 :ATTACH:
:PROPERTIES:
:ID:       7b2208c3-daad-46bf-942b-ff01a598491c
:END:
[[attachment:_20241228_140726screenshot.png]]


** 消融实验 :ATTACH:
:PROPERTIES:
:ID:       b92ebfdf-5818-4c1f-b63d-c3446a5f4880
:END:
[[attachment:_20241228_140744screenshot.png]]
组件：
- 通道注意力
- 并行卷积
- 卷积核大小


* 总结
** Conv
- 捕获局部特征
  # 卷积核在输入区域局部扫描，一次只处理输入的一个局部区域
  # 局部感知：人脑识别图像，不是一下子就识别整个图像的特征，而是先识别局部图像的特征，然后在高层次对局部进行综合操作，得到全局信息
- 特征提取
- 有效降低计算复杂度，同时保留关键的特征信息
** MLP
- 捕获全局特征
  #  源于全连接层：每个输出神经元都与上一层的所有输入神经元相连，即每个输出神经元可以收到输入的全局信息
** 图像划分成多个patch的目的
- 降低计算复杂度：减少输入序列长度
- 与transformer结构兼容：原来是处理文本的，每个patch相当于是一个token
- 增强模型全局感知能力：通过自注意力机制直接捕捉图像中任意两个patch之间的关系
  # 传统CNN中，感受野是逐步增加的，较低层次只能感受到局部区域的特征
- 将图像划分成小块：从较小区域逐步学习局部特征
- 处理任意分辨率的图像
