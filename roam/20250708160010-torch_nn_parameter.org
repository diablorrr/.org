:PROPERTIES:
:ID:       d689ec57-c76f-4e2f-8c84-c3edd8140b60
:END:
#+title: torch.nn.Parameter
#+filetags: pytorch

* torch.nn.Parameter [[https://blog.csdn.net/qq_43391414/article/details/120484239][csdn]]
1. 用于将 不可训练Tensor 变成 *可训练的parameter* ；并且将 该parameter *注册到宿主模型* 中（model.parameters()中会包含该parameter），在参数优化时 *自动一起优化*
2. parameter *本质是 Tensor*
3. nn.Parameter = nn.parameter.Parameter
   #+begin_src python
   import torch
   import torch.nn as nn
   a=torch.tensor([1,2],dtype=torch.float32)
   print(a)
   print(nn.Parameter(a))
   print(nn.parameter.Parameter(a))
   #+end_src

   #+name: 输出
   #+begin_example
   tensor([1., 2.])
   Parameter containing:
   tensor([1., 2.], requires_grad=True)
   Parameter containing:
   tensor([1., 2.], requires_grad=True)
   #+end_example

4. nn.Parameter(tensor) 和 对tensor使用requires_grad=True 的 *区别* ：
   在于 =1.= ，后者虽然也变成 可训练的parameter，但是 *不会注册到 model.parameters()* 中
   #+begin_src python
   class mod(nn.Module):
       def __init__(self):
           super(mod,self).__init__()
           self.w1=torch.tensor([1,2],dtype=torch.float32,requires_grad=True) # 带梯度的普通tensor
           a=torch.tensor([3,4],dtype=torch.float32)                          # parameter
           self.w2=nn.Parameter(a)
       def forward(self,inputs):
           o1=torch.dot(self.w1,inputs)
           o2=torch.dot(self.w2,inputs)
           return o1+o2

   model=mod()
   for p in model.parameters():
       print(p)
   #+end_src

#+name: 输出
#+begin_example
Parameter containing:
tensor([3., 4.], requires_grad=True)
#+end_example
