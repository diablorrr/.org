:PROPERTIES:
:ID:       84b76ad2-597e-40d3-9332-27c9700ece88
:END:
#+title: interview_project
#+filetags: other

* 虚拟数字人
*微调流程* ：[[https://xtx0o8yn7x.feishu.cn/docx/XjvpdaeQcoF8d3xhpbCcoOJNn9b][微调]]
1. 加载分词器​​（AutoTokenizer）预处理数据，使用分词器将文本编码为数字，得到数据集
2. 加载模型​​（AutoModelForCausalLM + BitsAndBytesConfig量化）：量化，减少显存占用
3. 配置训练参数​​（TrainingArguments）：训练的超参数（学习率、批次等）
4. 配置Lora参数（peft中的LoraConfig）：用于高效参数微调
5. SFTTrainer（训练参数 + Lora参数 + 数据集）开始训练

#+begin_comment
使用 hugging face 中的库：
1. transformers：基础模型库
   用的 AutoTokenizer：加载分词器，用于将文本编码为数字
   AutoModelForCausalLM：加载模型
   BitsAndBytesConfig：量化参数（减少显存占用 4位量化、8位量化）
   TrainingArguments：训练超参数（学习率、批次大小等）
2. peft：参数高效微调库，用的 LoRA
3. trl：强化学习微调库，用的 SFTTrainer（监督式微调的训练器）
#+end_comment

- [X] 项目用的是哪种微调方式，LoRA、Freeze（冻结预训练层）还是全量微调？为什么在虚拟数字人场景下选择这种方案？
  + 用的LoRA（低秩适配），将原始大权重矩阵拆为两个低秩矩阵A、B，训练时只训练A、B，推理时合并原始矩阵和A、B；训练量更少
  + 在终端设备（eg：智能屏）的环境下算力不强，对精确度要求不需要那么高，选择 LoRA 模型响应更快
- [X] 虚拟数字人需要适配特定场景（比如客服、教育）的交互风格，你们的微调数据集是怎么构建的？
  + 从博物馆官网上爬来，然后交给chatgpt生成角色对话，且保持角色语气一致性


*外挂知识库流程* ：[[https://xtx0o8yn7x.feishu.cn/docx/UOAadjjReoI6UBxPq4lcI9frnMb][外挂知识库]]
# 以下都是 langchain 的模块
1. 用 UnstructuredFileLoader 加载文件，将非结构化文件（eg：PDF、Word、TXT等）转换为统一格式（Document对象列表：List[Document]）
2. RecursiveCharacterTextSplitter 将长文本分块（chunk），确保分块后保留语义完整
3. HuggingFaceEmbeddings中的 text2vec-large-chinese模型 对chunk进行embedding，存储到向量库 FAISS（Facebook开源的向量库） 中
4. 用户查询时，查询首先变成向量的形式，在 向量库中进行相似度搜索，返回相似度 top k，大模型结合这个返回结果进行回答

#+begin_example python
# Document对象列表：List[Document]
[
    Document(
        page_content="小白的父亲是张三。",
        metadata={"source": "/root/knowledge.txt"}
    ),
    Document(
        page_content="Llama2是由Meta发布的开源大模型。",
        metadata={"source": "/root/knowledge.txt"}
    )
]
#+end_example

- [X] 为什么使用 FAISS？
  （facebook开源）主要因为与 langchain 集成，方便使用（很多api调用方便）
- [X] 为什么选用 text2vec-large-chinese模型？
  中文模型，langchain 集成，调用方便


** LoRA(低秩矩阵适配)原理
1. *秩 代表信息量* ：秩越低则该矩阵所含信息量越少
   #+begin_comment
   [1 2 3] 这个矩阵秩为2，所含信息多
   [4 5 6]

   [1 2 3] 这个矩阵秩为1，所含信息少（第2行可以用第1行表示）
   [2 4 6]
   #+end_comment

2. *大矩阵可以分解为两个低秩矩阵* ：虽然两个低秩矩阵的信息量变少了，但是训练量也变小了
   #+begin_comment
   [ 4  5  6]   [1]
   [ 8 10 12] = [2] x [4 5 6]
   [12 15 18]   [3]
   #+end_comment

3. 训练时将原始矩阵冻结，用两个低秩矩阵A、B表示原始矩阵，并训练A、B
   推理时 =最终权重矩阵 = 原始权重矩阵 + 低秩矩阵A * 低秩矩阵B=




* llfc项目
- [ ] 怎么做的用户登录？
- [ ] 登录的具体流程
- [ ] 登录和聊天分别用是协议？
- [ ] 登录用的http的哪个方法，为什么用post，不用get？
- [ ] 为什么用异步不用同步？
- [ ] 怎么设计多线程模式？
- [ ] 域名如何映射到ip
- [ ] 项目中用的是私域ip还是公网ip，如何获得一个公网ip
- [ ] 连接池的实现原理？（怎么设计连接数），当有大量请求时，如何处理连接超时问题？
- [ ] 怎么封装的http和tcp？
- [ ] 项目中微服务的拆分、职责边界怎么划分
- [ ] 聊天服务器如何实现的负载均衡、当有大量请求到来时，如何实现连接的均匀分布？
- [ ] 怎么测出单服务器的连接数的，连接数的瓶颈在哪儿？
- [ ] MySQL连接池的设计要点？
- [ ] GRPC在项目中解决的什么问题？
- [ ] redis缓存的应用场景
- [ ] 聊天过程中的数据如何产生，如何传递，如何存储
- [ ] 存储聊天数据的表如何设计，如何建立索引
- [ ] 如果离线消息过大，需要等所有数据发送完后再删除吗，如果发送过程中服务断掉，那下一次要重复发送吗
- [ ] redis如果遇到内存快满了，如何处理
- [ ] 讲项目做的好的一个点
- [ ] 大量用户连接时，负载的处理和断连的处理
- [ ] 客户端请求到达服务端的通信链路
- [ ] 数据库中用户密码的加密存储
- [ ] 高并发场景下单个连接数不足的问题具体讲讲
- [ ] 项目还有什么不足和改进点
- [ ] 有没有了解消息队列
- [ ] 多线程并发修改全局变量会有什么问题，如何解决? （除了加锁还有什么方法?)
- [ ] 为什么要选择项目中的这个网络库（asio），以及这些技术
- [ ] 如何存储大量的聊天信息，包括音频视频以及各种大文件
- [ ] grpc如何实现断线重连
- [ ] 两个客户端之间如何实现聊天功能？（grpc）
- [ ] 服务器如何确定用户连接到哪个聊天服务器？
- [ ] 如何实现发送验证码功能？
- [ ] 为什么需要网关服务，没有行不行？
- [ ] 如何获得服务器性能的？（测试）
- [ ] 如何解决大量tcp连接的性能问题？如何用户登陆后长时间没有请求（心跳机制），如何又突然要发消息了，需要重新登录吗？
- [ ] 在项目中用到的redis函数？
- [ ] redis在项目中的作用？
- [ ] redis常用的数据结构，redis是单线程还是多线程、为什么单线程他的效率还高？
- [ ] 为什么要把登录和聊天拆分成不同的服务器？
- [ ] 不同的聊天服务器是物理上的分离，还是逻辑上的分离？
- [ ] 每个聊天服务器都是一个单独的ip，如果一个服务器挂了、那么这个服务器上的客户端都会受到影响，那如何做到无损切换服务器？
- [ ] 项目中mysql主要存储哪些数据？
- [ ] 如何实现A发消息给B，服务器如何识别不同的客户端并实现消息转发？
- [ ] 客户端使用什么接口进行消息发送的？
- [ ] grpc用的什么协议？protobuf的作用？
- [ ] 具体的添加好友的业务怎么实现的？
- [ ] 主动添加方的消息推送是怎么实现的（因为被添加方同意时，主动添加方可能不在线）
- [ ] 同一账号，在设备上添加的好友，之后在b设备上登陆时，如何同步这个好友信息？
- [ ] 什么是asio，具体可以实现哪些功能？


** 客户端
- 与服务器的通信端口：
  1. 网关：8080
  2. chat1：8090
  3. chat2：8091

** 服务端
*网关服务器* ：处理客户端HTTP请求，根据请求内容的不同，将请求分发到不同的服务器，将响应结果返回给客户端
*状态服务器* ：存储聊天服务器的状态信息，主要是负载；网关服务器请求状态服务器（作为grpc服务端），状态服务器会返回负载较低的 聊天服务器的ip、端口、token
*验证服务器* ：（作为grpc服务端）通过uuid生成4位随机数作为验证码，存放在redis中设置过期时间，并发送邮件到用户邮箱，用户在Qt客户端填写信息发送到服务端后，与redis中存储的验证码对比
*聊天服务器* ：（作为grpc服务器），Qt客户端通过TCP连接到聊天服务器，聊天服务器，向聊天服务器发送消息，若

*** GateServer
监听端口、处理连接
HttpConnection类：数据的收发，解析HTTP请求，并交给LogicSystem处理，LogicSystem 中注册了一些业务相关的回调函数：登录、注册、获取验证码、重置密码

*** StatusServer
- 核心 ::
  1. 创建StatusServiceImpl对象，用于构建并启动grpc服务器，接收请求
  2. StatusServiceImpl返回负载最小的ChatServer的ip、host、token
- 辅助 ::
  1. ConfigMgr为StatusServiceImpl获取ChatServer的配置信息，并存入StatusServiceImpl
  2. RedisMgr缓存查询负载最小ChatServer时的信息：负载最小的服务器、登录的数量
- 细节 ::

*** ChatServer
- 核心 ::
  1. 创建ChatServiceImpl对象，用于构建并启动grpc服务器，用于与qt客户端通信
  2. CServer保管很多CSession，用会话id识别
  3. CSession用于读写数据，将读到的数据存放到LogicSystem的消息队列，将要写的数据先放到发送队列
  4. LogicSystem注册了相关逻辑的回调函数：登录、搜索用户、添加好友、认证好友
  5. LogicSystem处理消息队列中的消息，调用相应回调函数
  6. 登录：验证token
- 辅助 ::
  1. ConfigMgr
  2. AsioIOServicePool
  3. RedisMgr
- 细节 ::

** 实现
*redis线程池* ：创建 redis连接，并将 redis连接 放入queue中，后台起一个 check线程，每隔一段时间检查 redis连接 健康状态（PING），状态正常则放回连接池，状态不正常删除连接，并创建新连接放入连接池；取出和放回操作注意加锁，其次注意判断 线程池关闭的标志变量 b_stop_

** DONE io_context底层原理？
io_context是一个事件循环，内部存在任务队列，任务队列中存放着异步操作的回调函数、手动提交的任务；事件循环启动后会处理任务队列中的任务。

** DONE 如何封装的io_context(AsioIOServicePool.cpp)？
AsioIOServicePool(线程池)
每个线程内启动一个[[id:a853063a-5a85-4bc9-8afc-65731a28e27e][事件循环]]([[id:17d4394b-4f3d-479f-a51b-2f821387e81b][io_context]])，处理异步回调和手动提交的任务；为了防止任务队列空的时候，事件循环退出，使用work

** DONE 如何封装的mysql(MysqlMgr.cpp)？
MySqlPool(连接池) => 存储mysql连接 => 起一个线程实现心跳机制 => 保证连接存活
MysqlDao为数据访问层，对MySqlPool进行封装
MysqlMgr为业务访问层，对MysqlDao进行封装

** DONE 如何封装的redis(RedisMgr.cpp)？
RedisConPool(连接池) => 存储redis连接 => 起一个线程实现心跳机制[fn:3] => 保证连接存活
RedisMgr对RedisConPool进行封装，提供对redis操作的接口

** DONE ChatServer的CSession.cpp中为什么将发送的数据先放到发送队列？
这个队列是生产者消费者模型的实现，解耦了生产者和消费者的逻辑、支持多线程并发操作、利用队列的缓冲能力平衡双方速率差异；同时队列先进先出的特点保证了处理消息的顺序

** DONE ChatServer的LogicSystem.cpp中为什么将接收的数据放到消息队列？
这个队列是生产者消费者模型的实现，解耦了生产者和消费者的逻辑、支持多线程并发操作、利用队列的缓冲能力平衡双方速率差异；同时队列先进先出的特点保证了处理消息的顺序

** DONE 如何使用grpc进行数据传输的？
客户端创建channel，再用channel创建stub，使用stub通信
服务端继承Service并实现相应虚函数得到服务；再用ServerBuilder监听端口，并注册服务


* 百万并发reactor服务器
1. 主从reactor

2. 整个系统中上层注册回调函数给下层（上层 -> 下层），下层根据事件的不同而触发不同的回调（下层 -> 上层），这就好似上下层之间通信，事件的来源是 epoll；当事件触发时，从下层到上层 层层回调
   #+begin_comment 示例
   上层：EchoServer
   下层：TcpServer
   再下层：....
   #+end_comment

3. 主线程对应 主EventLoop，主线程的主EventLoop与Acceptor绑定，负责监听和接收新连接
   其他IO线程一对一 从EventLoop，每个从EventLoop对应几个Connection
   EchoServer中线程池里都是 WORK线程
   #+begin_example cpp
   // 每个Connection根据内部的客户端socket的fd通过哈希函数打到不同的从事件循环上，因此多个Connection对应到一个EventLoop
   spConnection conn(new Connection(subloops_[clientsock->fd()%threadnum_].get(),std::move(clientsock)));
   #+end_example

EchoServer 中的线程池是 工作线程，该线程池中的 任务队列，处理 业务任务

** 问答
*** 介绍下你的项目
- 项目介绍           :: 这个一个基于 Reactor 模型的高并发 TCP服务器框架，在这个TCP服务器框架上封装了 EchoServer；以 =epoll + 非阻塞IO= 实现百万级连接能力，其次它具有 事件驱动、线程安全、易扩展 等特点
- 整体架构与线程模型 :: 主线程中运行主事件循环，负责监听、接受新连接；IO线程池中运行从事件循环，负责连接的IO；可选的工作线程池负责业务处理（计算密集型任务）

*** 从 accept 新连接到连接读写、消息处理、再到回包的完整链路描述，包括涉及的对象和回调绑定位置
- 首先这是一个 EchoServer，是对 TcpServer 的封装，TcpServer 中有 一个主事件循环、若干从事件循环，Acceptor绑定在主事件循环，监听和接受新连接，Connection绑定在从事件循环；one thread one loop
- 新连接： *「主线程」* Acceptor::newconnection()接受连接，构造客户端socket，回调TcpServer::newconnection()，在从EventLoop中选择一个，并创建Connection，Connection内部有客户端socket和clientchannel并注册读写/错误/关闭回调，后续Channel::handleevent()触发对应回调
- 消息处理/回包： *「IO线程A」* clientchannel触发读事件，回调Connection::onmessage()，Connection::onmessage()读取数据到自己的缓冲区，回调TcpServer::onmessage()，TcpServer::onmessage()回调上层EchoServer::HandleMessage()，EchoServer::HandleMessage()中将 =任务std::bind(&EchoServer::OnMessage,this,conn,message)= 放入工作线程池的任务队列， *「WORK线程A」* 调用任务EchoServer::OnMessage，EchoServer::OnMessage调用Connection::send()发送响应，Connection::send()中判断出当前执行该函数的线程是「WORK线程A」，于是通过EventLoop::queueinloop将任务放入conn对应的EventLoop::taskqueue中，EventLoop::queueinloop内通过eventfd唤醒「IO线程A」， *「IO线程A」* 处理taskqueue

*** 为什么要用 eventfd 唤醒而不使用其他手段？
- 简单只需1个fd，与epoll无缝集成
  pipe                  ：需要2个fd
  pthread条件变量/信号量：不能纳入epoll的等待集合
  signal                ：全局性强
  timerfd               ：专门做计时器事件，不适合作为跨线程“任务到达”的通知
  C++11互斥锁/条件变量  ：与epoll集成不好，会造成双重等待（既等条件变量，又等epoll_wait）

*** Connection在回调过程中为什么要用shared_from_this()，用this行吗，或者 new 一个Connection的共享指针？
# Connection::onmessagecallback_(shared_from_this(),message);
- 用 this 不行，因为 this 不能增加 Connection 的引用计数，可能在回调过程中 Connection 被意外释放；new一个Connection的共享指针也不行，因为这样创建的共享指针引用计数与正在管理Connection的共享指针引用计数不同步（创建了新的shared_ptr），使用shared_from_this() 就能共享正在管理Connection的共享指针的引用计数

*** 如何做消息拆包？Buffer::pickmessage() 三种分隔方式各自适用什么场景？
- 消息 = 消息头(固定4字节) + 消息体；读取消息头获取长度信息，根据长度信息读取消息体
- 0：无分割符，适合定长/持续流（eg：音视频）
  1：4字节消息头（默认），处理粘包问题
  2：\r\n\r\n分隔，处理HTTP

*** EventLoop::stop_ 是原子变量，停止时如何保证不会丢任务？如何优雅退出所有 subloops_ 与工作线程？
- EventLoop::stop() 会将 stop_ 设置为 true，然后通过 eventfd 通知 EventLoop 将任务队列中的任务全部处理，同时 stop_=true 后，EventLoop::run() 中 while循环停止，也就是 事件循环停止了
- 设计了一个 ThreadPool::stop()，将 原子变量 stop_设置为true，然后唤醒全部阻塞的线程，等待它们执行完成；被唤醒的线程发现 stop_=true且任务队列为空就直接return，否则先将任务做完，下次while循环退出
  *补充* ：不直接在析构函数中设计这个逻辑是为了增加灵活性，用户可以直接调用 stop() 来手动停止线程池；ThreadPool::stop() 中一开始设计 if(stop_) return; 是因为join过的线程不能再join
  #+begin_example cpp
  void ThreadPool::stop()
  {
      if (stop_) return;       // 被join过的线程不能再join的，否则会调用terminate；eg：第一次stop()，对所有线程join了；第二次 stop 若没有这行，则再次join，则程序终止
      stop_ = true;
      condition_.notify_all();  // 唤醒全部的线程。
      // 等待全部线程执行完任务后退出。
      for (std::thread &th : threads_)
          th.join();
  }
  #+end_example

*** 为什么 EventLoop 里集成了 timerfd_？EventLoop::conns_ 和 TcpServer::conns_ 都维护了连接，为什么要两份？
- 一个 从EventLoop 管理多个 Connection，通过集成 timerfd_ 来定期检查 Connection 是否超时，将超时Connection从EventLoop中删除，并从TcpServer中删除
  #+begin_comment
  *具体来说* ：EventLoop中继承 timerfd、timerchannel 周期性触发 handletimer()，handletimer()设置重新计时并遍历所有 Connection，若超时，则从EventLoop::conns中删除，回调TcpServer::removeconn，TcpServer::removeconn将Connection从TcpServer::conns中删除
  #+end_comment
- TcpServer::conns 用于管理所有的Connection（eg：统一关闭），EventLoop::conns用于管理局部Connection（eg：空闲连接检测）
  #+begin_comment
  假设将空闲连接检测交给TcpServer::conns，则TcpServer性能不好，因为需要定期完整检查全部Connection，而TcpServer的职责主要是监听和接受新连接
  #+end_comment

*** 有无考虑进程级信号处理（如 SIGINT）触发所有 loops、ThreadPool 的优雅退出？
- 有进程级的信号处理，捕获到 SIGINT、SIGTERM时，调用自定义的Stop函数
  #+begin_example cpp
  signal(SIGTERM,Stop);    // 信号15，系统kill或killall命令默认发送的信号。
  signal(SIGINT,Stop);     // 信号2，按Ctrl+C发送的信号。

  // Stop函数中
  echoserver->Stop();
  delete echoserver;
  #+end_example

*** 实现主从 Reactor 吗？mainloop_ 和 subloops_ 的职责边界是什么？新连接如何在多个 EventLoop 之间分配（策略/理由），支持连接迁移吗？
- 主事件循环负责 监听/接受 新连接（Acceptor绑定在主事件循环），从事件循环负责Connection的IO事件、超时管理（事件循环内有个定时器，每隔一段时间检查Connection）
- 使用 哈希方法，简单且无锁（eg：轮询按顺序分配连接，因此可能需要锁）

*** 为什么 Socket、Channel 在不同地方用 unique_ptr，而连接放在 std::map<int, spConnection>？
- socket表示网络连接，channel表示事件，每个 Connection 独占各自的socket、channel，因此使用 unique_ptr；而 Connection 存在于 TcpServer 和 EventLoop 中，且在一些回调函数中需要传递 Connection，因此使用 shared_ptr，以便在回调链路中延续生命周期

*** ThreadPool 主要用于 IO 线程还是工作线程？threadtype_ 的意图是什么？
- 都有，用 threadtype_ 标识线程池类型

*** 用什么方式压测（客户端、QPS、RT、连接数、CPU/内存）？
- 虚拟机：4核CPU、2g内存 Ubuntu系统
  sh脚本中在后台起了30个客户端，每个客户端发送10万个请求，计算下来大概1秒10万
- autodl重庆A区：32核CPU 120g内存 Ubuntu系统
  sh脚本中在后台起了30个客户端，每个客户端发送100万个请求，关闭可选的工作线程池，IO线程数改为30（也就是整个程序31个线程），计算下来QPS=100万
  + top命令看CPU和内存
  + QPS=请求/每秒，QPS是每秒查询率

*** Channel::useet() 配合 EPOLLET 时，如何避免丢事件、确保把缓冲区读到 EAGAIN？写事件何时启用/关闭？

*** 连接关闭/错误的路径是什么？如何避免重复关闭？如何确保回调顺序和资源回收正确？

*** EventLoop::setepolltimeoutcallback() 和 settimercallback() 的典型用途？二者在你业务中分别做什么？

*** 真正上到百万连接需要哪些内核与系统调优？你准备调整哪些参数、为何？

*** 零拷贝与大报文：发送大文件/大报文时是否考虑 sendfile/splice/mmap/writev？当前 Buffer 设计的瓶颈在哪里？

*** 如何处理对端半关闭（read 返回 0）与 RST？多次触发关闭路径会怎样？

* Footnotes

[fn:3] 每隔一定时间，检查所有连接一次
[fn:2] 子类构造函数私有，但父类需要构造子类
[fn:1] 父类调用子类构造，子类构造会调用父类构造；不希望外部直接通过Singleton<T>()实例化
